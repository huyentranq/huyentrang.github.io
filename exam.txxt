[
{
  "id": "architecture",
  "category": "spark",
  "title": "Architecture",
  "description": "Tổng quan kiến trúc Spark: Driver, Executor, DAG, Cluster Manager.",
  "details": [
    {
      "type": "text",
      "content": "Apache Spark sử dụng mô hình kiến trúc master-slave. \nTrong đó, Driver Program chịu trách nhiệm khởi tạo SparkContext, gửi mã (có thể là file jar) và phụ thuộc đến các Executor để thực thi. \nKhi gặp Action như `count()` hoặc `collect()`, Spark tạo Job và thông qua DAG Scheduler để phân tách thành các stage, sau đó Task Scheduler chia nhỏ ra nhiều task và gửi đến các Executor nằm trên Worker Node.\n- Cluster Manager sẽ quản lý tài nguyên và điều phối thực thi. Kiến trúc này giúp Spark thực thi song song và phân tán hiệu quả."
    },
    {
      "type": "code",
      "content": "SparkContext -> DAG Scheduler -> Task Scheduler -> Executors"
    },
    {
      "type": "image",
      "src": "/images/spark/architecture.png",
      "alt": "Kiến trúc Spark"
    }
  ]
}
,
  {
    "id": "groupby",
    "category": "spark",
    "title": "GroupBy Example",
    "description": "Ví dụ sử dụng GroupBy trong Spark.",
    "details": [
      { "type": "text", "content": "GroupBy là một trong các phép biến đổi quan trọng trong Spark." },
      {
        "type": "code",
        "content": "val result = data.groupBy(\"key\").agg(sum(\"value\"))"
      },
      {
        "type": "image",
        "src": "/images/spark-groupby.png",
        "alt": "GroupBy Example"
      }
    ]
  },
  {
  "id": "spark-filter",
  "category": "spark",
  "title": "Filter",
  "description": "Lọc dữ liệu với điều kiện logic trong Spark.",
  "details": [
    {
      "type": "code",
      "content": "df.filter(col('Item_Fat_Content') == 'Regular').display()"
    },
    {
      "type": "code",
      "content": "df.filter((col('Item_Type') == 'Soft Drinks') & (col('Item_Weight') < 10)).display()"
    },
    {
      "type": "code",
      "content": "df.filter((col('Outlet_Size').isNull()) & (col('Outlet_Location_Type').isin('Tier 1','Tier 2'))).display()"
    }
  ]
},
{
  "id": "spark-withcolumn",
  "category": "spark",
  "title": "WithColumn & Rename",
  "description": "Tạo cột mới hoặc đổi tên cột trong DataFrame.",
  "details": [
    {
      "type": "code",
      "content": "df.withColumnRenamed('Item_Weight','Item_Wt').display()"
    },
    {
      "type": "code",
      "content": "df.withColumn('multiply', col('Item_Weight') * col('Item_MRP')).display()"
    },
    {
      "type": "code",
      "content": "df.withColumn('flag', lit('new')).display()"
    },
    {
      "type": "code",
      "content": "df = df.withColumn('Item_Fat_Content', regexp_replace(col('Item_Fat_Content'), 'Regular', 'Reg')).withColumn('Item_Fat_Content', regexp_replace(col('Item_Fat_Content'), 'Low Fat', 'Lf'))"
    }
  ]
},
  {
    "id": "spark-join",
    "category": "spark",
    "title": "Join DataFrames",
    "description": "Kết hợp hai DataFrame bằng các phép join.",
    "details": [
      { "type": "text", "content": "Spark hỗ trợ các loại join như inner, left, right, full." },
      {
        "type": "code",
        "content": "df1.join(df2, df1.id == df2.id, 'inner')"
      },
      {
        "type": "image",
        "src": "/images/spark-join.png",
        "alt": "Join DataFrames"
      }
    ]
  },
  {
    "id": "spark-udf",
    "category": "spark",
    "title": "User Defined Functions (UDF)",
    "description": "Tạo và sử dụng UDF trong Spark.",
    "details": [
      { "type": "text", "content": "UDF cho phép bạn định nghĩa hàm tùy chỉnh để áp dụng lên DataFrame." },
      {
        "type": "code",
        "content": "val myUDF = udf((x: String) => x.toUpperCase())\ndf.withColumn(\"upper_col\", myUDF(col(\"col_name\")))"
      }
    ]
  },
  {
  "id": "spark-drop",
  "category": "spark",
  "title": "Drop & Duplicates",
  "description": "Xóa cột hoặc loại bỏ dữ liệu trùng trong DataFrame.",
  "details": [
    {
      "type": "code",
      "content": "df.drop('Item_Visibility').display()"
    },
    {
      "type": "code",
      "content": "df.drop('Item_Visibility', 'Item_Type').display()"
    },
    {
      "type": "code",
      "content": "df.dropDuplicates().display()"
    },
    {
      "type": "code",
      "content": "df.drop_duplicates(subset=['Item_Type']).display()"
    },
    {
      "type": "code",
      "content": "df.distinct().display()"
    }
  ]
},
  {
    "id": "spark-agg",
    "category": "spark",
    "title": "Aggregation Functions",
    "description": "Sử dụng các hàm tổng hợp trong Spark.",
    "details": [
      { "type": "text", "content": "Các hàm như count, sum, avg, min, max thường được sử dụng trong Spark." },
      {
        "type": "code",
        "content": "df.groupBy(\"column\").agg(count(\"*\"), sum(\"value\"))"
      },
      {
        "type":"code",
        "content": "df.groupBy(\"column\").agg(avg(\"value\"), min(\"value\"), max(\"value\"))"

      },
      {
        "type":"code",
        "content": "df.groupBy(\"column\").agg(collect_list(\"value\"), collect_set(\"value\"))"
      }
    ]
  },
  {
    "id": "spark-sql",
    "category": "spark",
    "title": "Spark SQL",
    "description": "Sử dụng Spark SQL để truy vấn dữ liệu.",
    "details": [
      { "type": "text", "content": "Spark SQL cho phép bạn sử dụng  SQL để truy vấn dữ liệu trong DataFrame." },
      {
        "type": "code",
        "content": "df.createOrReplaceTempView(\"my_table\")\nspark.sql(\"SELECT * FROM my_table WHERE column = 'value'\")"
      }
    ]
  }

]
