[
{
  "id": "architecture",
  "category": "spark",
  "title": "Spark Architecture",
  "description": "Ki·∫øn tr√∫c t·ªïng quan c·ªßa Apache Spark, bao g·ªìm Driver, Executor, DAG Scheduler v√† Cluster Manager.",
  "details": [
        {
      "type": "image",
      "src": "/images/spark/architecture.png",
      "alt": "Ki·∫øn tr√∫c t·ªïng th·ªÉ Spark"
    },
    {
      "type": "text",
      "content": "Apache Spark l√† m·ªôt framework x·ª≠ l√Ω d·ªØ li·ªáu ph√¢n t√°n m·∫°nh m·∫Ω, ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ x·ª≠ l√Ω l∆∞·ª£ng l·ªõn d·ªØ li·ªáu m·ªôt c√°ch nhanh ch√≥ng v√† hi·ªáu qu·∫£. Ki·∫øn tr√∫c c·ªßa Spark bao g·ªìm c√°c th√†nh ph·∫ßn ch√≠nh sau:"
    },
    {
      "type": "text",
      "content": "Apache Spark s·ª≠ d·ª•ng ki·∫øn tr√∫c master-slave, trong ƒë√≥:\n- **Driver Program**: ƒëi·ªÅu khi·ªÉn to√†n b·ªô ti·∫øn tr√¨nh, t·∫°o `SparkContext`, g·ª≠i m√£ ch∆∞∆°ng tr√¨nh v√† gi√°m s√°t ti·∫øn ƒë·ªô.\n- **Cluster Manager**: qu·∫£n l√Ω t√†i nguy√™n (RAM, CPU) gi·ªØa c√°c node v√† ph√¢n b·ªï cho c√°c Executor.\n- **Executors**: ch·∫°y tr√™n Worker Node, th·ª±c thi c√°c task v√† l∆∞u tr·ªØ d·ªØ li·ªáu trung gian.\n- **DAG Scheduler & Task Scheduler**: DAG Scheduler chia nh·ªè job th√†nh c√°c stage, Task Scheduler ph√¢n chia th√†nh task v√† g·ª≠i t·ªõi c√°c Executor th·ª±c thi song song."
    },
    {
      "type": "text",
      "content": "### üîß Ki·∫øn tr√∫c th·ª±c thi n·ªôi b·ªô trong Apache Spark:\n1. **Driver Program**: ƒêi·ªÉm kh·ªüi ƒë·∫ßu c·ªßa ·ª©ng d·ª•ng, t·∫°o `SparkSession`, x√¢y d·ª±ng RDD, g·ª≠i Action (nh∆∞ `count()`) ƒë·ªÉ k√≠ch ho·∫°t job.\n2. **SparkContext**: Giao ti·∫øp gi·ªØa Driver v√† c√°c th√†nh ph·∫ßn b√™n d∆∞·ªõi, g·ª≠i DAG ƒë·∫øn DAG Scheduler.\n3. **DAG Scheduler**: Ph√¢n t√≠ch DAG, chia th√†nh stage, g·ª≠i ƒë·∫øn Task Scheduler.\n4. **Task Scheduler**: Chia nh·ªè stage th√†nh c√°c task theo partition v√† g·ª≠i ƒë·∫øn Cluster Manager.\n5. **Cluster Manager**: (YARN, Mesos, Kubernetes...) qu·∫£n l√Ω t√†i nguy√™n, ph√¢n b·ªï executor tr√™n c√°c node.\n6. **Executors**: Ch·∫°y task th·ª±c t·∫ø, l∆∞u d·ªØ li·ªáu trung gian v√† tr·∫£ k·∫øt qu·∫£ v·ªÅ cho Driver."
    },
    {
      "type": "text",
      "content": "**Lu·ªìng d·ªØ li·ªáu t·ªïng qu√°t:**\nDriver ‚Üí SparkContext ‚Üí DAG Scheduler ‚Üí Task Scheduler ‚Üí Cluster Manager ‚Üí Executor ‚Üí K·∫øt qu·∫£\n\n- Vi·ªác th·ª±c thi ƒë∆∞·ª£c b·∫Øt ƒë·∫ßu **ch·ªâ khi g·∫∑p Action**, nh·ªù v√†o c∆° ch·∫ø **Lazy Evaluation**.\n- DAG gi√∫p x√°c ƒë·ªãnh ch√≠nh x√°c c√°c ph√©p bi·∫øn ƒë·ªïi v√† t·ªëi ∆∞u h√≥a vi·ªác th·ª±c thi song song."
    },
    {
      "type": "code",
      "content": "SparkContext -> DAG Scheduler -> Task Scheduler -> Cluster Manager -> Executors"
    }

  ]
}
,
{
  "id": "concepts",
  "category": "spark",
  "title": "Spark Core Concepts",
  "description": "T·ªïng h·ª£p c√°c kh√°i ni·ªám c∆° b·∫£n v√† quan tr·ªçng trong Spark.",
  "details": [
    {
      "type": "text",
      "content": "- **RDD (Resilient Distributed Dataset)**: C·∫•u tr√∫c d·ªØ li·ªáu ph√¢n t√°n, b·∫•t bi·∫øn v√† h·ªó tr·ª£ thao t√°c song song. Cho ph√©p l·∫≠p tr√¨nh ph√¢n t√°n tr·ª±c ti·∫øp, nh∆∞ng kh√¥ng t·ªëi ∆∞u b·∫±ng DataFrame.\n\n- **DataFrame**: T·∫≠p d·ªØ li·ªáu d·∫°ng b·∫£ng c√≥ schema. D·ªÖ d√πng, h·ªó tr·ª£ API nh∆∞ SQL v√† ƒë∆∞·ª£c t·ªëi ∆∞u b·ªüi Catalyst optimizer. Th∆∞·ªùng d√πng thay RDD ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu hi·ªáu qu·∫£ h∆°n.\n\n- **Dataset** (Scala/Java): K·∫øt h·ª£p ∆∞u ƒëi·ªÉm c·ªßa RDD (type safety) v√† DataFrame (t·ªëi ∆∞u h√≥a). Kh√¥ng c√≥ trong PySpark.\n\n- **Transformation**: C√°c ph√©p bi·∫øn ƒë·ªïi lazy (kh√¥ng th·ª±c thi ngay), nh∆∞ `filter`, `map`, `select`, `groupBy`. Ch·ªâ t·∫°o DAG ƒë·ªÉ ch·ªù action.\n\n- **Action**: G·ªçi th·ª±c thi DAG, v√≠ d·ª• `show()`, `collect()`, `count()`, `take()`.\n\n- **Lazy Evaluation**: Spark tr√¨ ho√£n t√≠nh to√°n cho ƒë·∫øn khi c√≥ action, gi√∫p t·ªëi ∆∞u h√≥a pipeline x·ª≠ l√Ω.\n\n- **DAG (Directed Acyclic Graph)**: Spark t·∫°o DAG th·ªÉ hi·ªán lu·ªìng ph·ª• thu·ªôc gi·ªØa c√°c ph√©p bi·∫øn ƒë·ªïi. DAG Scheduler s·∫Ω chia DAG th√†nh c√°c stage.\n\n- **Stage & Task**: M·ªôt job ƒë∆∞·ª£c chia th√†nh nhi·ªÅu stage (t·∫≠p h·ª£p task). M·ªói task x·ª≠ l√Ω 1 partition d·ªØ li·ªáu. C√°c task ƒë∆∞·ª£c ph√¢n ph·ªëi song song ƒë·∫øn Executor.\n\n- **SparkSession & SparkContext**: `SparkSession` l√† entry point hi·ªán ƒë·∫°i cho m·ªçi thao t√°c Spark (DataFrame, SQL...). `SparkContext` l√† n·ªÅn t·∫£ng c≈© d√πng cho RDD.\n\n- **Cluster Manager**: Th√†nh ph·∫ßn qu·∫£n l√Ω t√†i nguy√™n c·ªßa cluster. Spark c√≥ th·ªÉ ch·∫°y tr√™n Standalone, YARN, Mesos ho·∫∑c Kubernetes."
    }
  ]
}
,
  {
    "id": "filter",
    "category": "spark",
    "title": "Filter DataFrame",
    "description": "L·ªçc d·ªØ li·ªáu theo ƒëi·ªÅu ki·ªán logic trong PySpark.",
    "details": [
      {
        "type": "code",
        "content": "df.filter(col('Item_Fat_Content') == 'Regular').show()"
      },
      {
        "type": "code",
        "content": "df.filter((col('Item_Type') == 'Soft Drinks') & (col('Item_Weight') < 10)).show()"
      },
      {
        "type": "code",
        "content": "df.filter((col('Outlet_Size').isNull()) & (col('Outlet_Location_Type').isin('Tier 1', 'Tier 2'))).show()"
      }
    ]
  },
  {
    "id": "udf",
    "category": "spark",
    "title": "User Defined Functions (UDF)",
    "description": "T·∫°o v√† s·ª≠ d·ª•ng h√†m t√πy ch·ªânh trong PySpark.",
    "details": [
      {
        "type": "text",
        "content": "Khi kh√¥ng th·ªÉ x·ª≠ l√Ω logic b·∫±ng h√†m c√≥ s·∫µn, b·∫°n c√≥ th·ªÉ ƒë·ªãnh nghƒ©a h√†m ri√™ng b·∫±ng UDF."
      },
      {
        "type": "code",
        "content": "from pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\nto_upper = udf(lambda x: x.upper(), StringType())\ndf.withColumn(\"upper_col\", to_upper(col(\"col_name\"))).show()"
      }
    ]
  },
  {
    "id": "select",
    "category": "spark",
    "title": "Select & Show",
    "description": "Ch·ªçn c·ªôt v√† hi·ªÉn th·ªã d·ªØ li·ªáu trong PySpark DataFrame.",
    "details": [
      {
        "type": "code",
        "content": "df.select('Item_Weight', 'Item_MRP').show()"
      },
      {
        "type": "code",
        "content": "df.select(col('Item_Weight'), col('Item_MRP')).show()"
      },
      {
        "type": "code",
        "content": "df.selectExpr('Item_Weight * Item_MRP as Total').show()"
      }
    ]
  },
  {
  "id": "join",
  "category": "spark",
  "title": "Join DataFrames",
  "description": "K·∫øt h·ª£p hai DataFrame b·∫±ng c√°c lo·∫°i join kh√°c nhau trong PySpark.",
  "details": [
    {
      "type": "text",
      "content": "PySpark h·ªó tr·ª£ nhi·ªÅu lo·∫°i `join` nh∆∞:\n\n- **inner**: Ch·ªâ gi·ªØ l·∫°i c√°c d√≤ng c√≥ kh√≥a tr√πng kh·ªõp ·ªü c·∫£ hai DataFrame.\n- **left** (left outer): Gi·ªØ l·∫°i t·∫•t c·∫£ d√≤ng c·ªßa DataFrame b√™n tr√°i, k·∫øt h·ª£p d·ªØ li·ªáu t·ª´ b√™n ph·∫£i n·∫øu c√≥.\n- **right** (right outer): T∆∞∆°ng t·ª± nh∆∞ left, nh∆∞ng gi·ªØ l·∫°i to√†n b·ªô d√≤ng t·ª´ DataFrame b√™n ph·∫£i.\n- **outer** (full outer): Gi·ªØ l·∫°i t·∫•t c·∫£ d√≤ng t·ª´ c·∫£ hai b·∫£ng, ph·∫ßn kh√¥ng kh·ªõp s·∫Ω c√≥ gi√° tr·ªã `null`.\n- **left_semi**: Gi·ªØ l·∫°i c√°c d√≤ng t·ª´ DataFrame b√™n tr√°i m√† c√≥ kh√≥a kh·ªõp v·ªõi b√™n ph·∫£i, nh∆∞ng **kh√¥ng** l·∫•y c·ªôt t·ª´ b√™n ph·∫£i.\n- **left_anti**: Gi·ªØ l·∫°i c√°c d√≤ng t·ª´ b√™n tr√°i m√† **kh√¥ng** c√≥ kh√≥a kh·ªõp v·ªõi b√™n ph·∫£i."
    },
    {
      "type": "code",
      "content": "df1.join(df2, df1.id == df2.id, 'inner').show()  # inner join"
    },
    {
      "type": "code",
      "content": "df1.join(df2, df1.id == df2.id, 'left').show()  # left outer join"
    },
    {
      "type": "code",
      "content": "df1.join(df2, df1.id == df2.id, 'right').show()  # right outer join"
    },
    {
      "type": "code",
      "content": "df1.join(df2, df1.id == df2.id, 'outer').show()  # full outer join"
    },
    {
      "type": "code",
      "content": "df1.join(df2, df1.id == df2.id, 'left_semi').show()  # ch·ªâ l·∫•y c·ªôt t·ª´ df1"
    },
    {
      "type": "code",
      "content": "df1.join(df2, df1.id == df2.id, 'left_anti').show()  # d√≤ng kh√¥ng kh·ªõp t·ª´ df1"
    },
    {
      "type": "text",
      "content": "**L∆∞u √Ω:** C·ªôt `id` ph·∫£i c√≥ c√πng t√™n ho·∫∑c c·∫ßn ƒë∆∞·ª£c tham chi·∫øu r√µ r√†ng (`df1.id == df2.id`). Trong tr∆∞·ªùng h·ª£p c·ªôt b·ªã tr√πng t√™n, n√™n d√πng `.alias()` ƒë·ªÉ tr√°nh xung ƒë·ªôt c·ªôt sau khi join."
    }
  ]
},
{
  "id": "withcolumn",
  "category": "spark",
  "title": "WithColumn & Rename",
  "description": "Th√™m c·ªôt m·ªõi ho·∫∑c ƒë·ªïi t√™n c·ªôt trong PySpark DataFrame.",
  "details": [
    {
      "type": "code",
      "content": "// ƒê·ªïi t√™n c·ªôt 'Item_Weight' th√†nh 'Item_Wt'\ndf.withColumnRenamed('Item_Weight','Item_Wt').show()"
    },
    {
      "type": "code",
      "content": "// Th√™m c·ªôt m·ªõi 'multiply' l√† t√≠ch c·ªßa hai c·ªôt 'Item_Weight' v√† 'Item_MRP'\ndf.withColumn('multiply', col('Item_Weight') * col('Item_MRP')).show()"
    },
    {
      "type": "code",
      "content": "// Th√™m c·ªôt 'flag' c√≥ gi√° tr·ªã m·∫∑c ƒë·ªãnh l√† chu·ªói 'new'\ndf.withColumn('flag', lit('new')).show()"
    },
    {
      "type": "code",
      "content": "// Thay th·∫ø gi√° tr·ªã trong c·ªôt 'Item_Fat_Content':\n// 'Regular' -> 'Reg', 'Low Fat' -> 'Lf'\ndf = df.withColumn('Item_Fat_Content', regexp_replace(col('Item_Fat_Content'), 'Regular', 'Reg'))\n         .withColumn('Item_Fat_Content', regexp_replace(col('Item_Fat_Content'), 'Low Fat', 'Lf'))"
    }
  ]
}
,
  {
    "id": "groupby",
    "category": "spark",
    "title": "GroupBy & Aggregation",
    "description": "S·ª≠ d·ª•ng groupBy v√† c√°c h√†m t·ªïng h·ª£p trong PySpark.",
    "details": [
      {
        "type": "code",
        "content": "df.groupBy(\"column\").agg(count(\"*\"), sum(\"value\")).show()"
      },
      {
        "type": "code",
        "content": "df.groupBy(\"column\").agg(avg(\"value\"), min(\"value\"), max(\"value\")).show()"
      },
      {
        "type": "code",
        "content": "df.groupBy(\"column\").agg(collect_list(\"value\"), collect_set(\"value\")).show()"
      },
      {
        "type": "image",
        "src": "/images/spark-groupby.png",
        "alt": "GroupBy trong PySpark"
      }
    ]
  },
 
  {
    "id": "drop",
    "category": "spark",
    "title": "Drop & Remove Duplicates",
    "description": "X√≥a c·ªôt v√† lo·∫°i b·ªè b·∫£n ghi tr√πng l·∫∑p trong DataFrame.",
    "details": [
      {
        "type": "code",
        "content": "df.drop('Item_Visibility').show()"
      },
      {
        "type": "code",
        "content": "df.drop('Item_Visibility', 'Item_Type').show()"
      },
      {
        "type": "code",
        "content": "df.dropDuplicates().show()"
      },
      {
        "type": "code",
        "content": "df.drop_duplicates(subset=['Item_Type']).show()"
      },
      {
        "type": "code",
        "content": "df.distinct().show()"
      }
    ]
  },
  {
    "id": "spark-sql",
    "category": "spark",
    "title": "Spark SQL",
    "description": "S·ª≠ d·ª•ng SQL ƒë·ªÉ truy v·∫•n d·ªØ li·ªáu trong DataFrame.",
    "details": [
      {
        "type": "text",
        "content": "Spark SQL cho ph√©p b·∫°n ƒëƒÉng k√Ω DataFrame nh∆∞ m·ªôt b·∫£ng t·∫°m th·ªùi v√† s·ª≠ d·ª•ng SQL ƒë·ªÉ truy v·∫•n."
      },
      {
        "type": "code",
        "content": "df.createOrReplaceTempView(\"my_table\")\nspark.sql(\"SELECT * FROM my_table WHERE column = 'value'\").show()"
      }
    ]
  },

{
  "id": "kafka-intro",
  "category": "kafka",
  "title": "Kafka Architecture",
  "description": "T·ªïng quan ki·∫øn tr√∫c Kafka v√† c√°c th√†nh ph·∫ßn c·ªët l√µi.",
  "details": [
    {
      "type": "text",
      "content": "Kafka l√† h·ªá th·ªëng **pub-sub** (publisher-subscriber) ph√¢n t√°n, gi√∫p truy·ªÅn t·∫£i d·ªØ li·ªáu theo th·ªùi gian th·ª±c gi·ªØa c√°c h·ªá th·ªëng. D∆∞·ªõi ƒë√¢y l√† c√°c th√†nh ph·∫ßn ch√≠nh:\n\n- **Cluster**: G·ªìm nhi·ªÅu **broker** ‚Äì l√† c√°c m√°y ch·ªß x·ª≠ l√Ω song song v√† ph√¢n ph·ªëi d·ªØ li·ªáu.\n- **Broker**: L∆∞u tr·ªØ v√† x·ª≠ l√Ω c√°c **partition** c·ªßa c√°c **topic**. M·ªói broker c√≥ th·ªÉ l∆∞u nhi·ªÅu partition.\n- **Topic**: K√™nh logic ch·ª©a c√°c message c√πng lo·∫°i (order, user, log...). M·ªói topic chia nh·ªè th√†nh nhi·ªÅu **partition**.\n- **Partition**: M·ªôt ph·∫ßn c·ªßa topic. M·ªói partition ch·ªâ ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi ƒë√∫ng 1 consumer trong 1 group t·∫°i m·ªôt th·ªùi ƒëi·ªÉm.\n- **Offset**: V·ªã tr√≠ (index) x√°c ƒë·ªãnh th·ª© t·ª± c·ªßa m·ªói message trong partition.\n- **Message**: ƒê∆°n v·ªã d·ªØ li·ªáu nh·ªè nh·∫•t g·ªìm key, value, offset, timestamp.\n- **Producer**: G·ª≠i d·ªØ li·ªáu v√†o topic. Kafka t·ª± ƒë·ªông x√°c ƒë·ªãnh partition ho·∫∑c s·ª≠ d·ª•ng key ƒë·ªÉ ph√¢n chia.\n- **Consumer**: ƒêƒÉng k√Ω l·∫Øng nghe topic, ƒë·ªçc d·ªØ li·ªáu t·ª´ partition t∆∞∆°ng ·ª©ng trong broker.\n- **ZooKeeper / KRaft**: D√πng ƒë·ªÉ qu·∫£n l√Ω metadata cluster, gi√°m s√°t broker, ch·ªçn leader n·∫øu broker ch·∫øt.\n- **Replication**: M·ªói partition c√≥ 1 b·∫£n ch√≠nh (**leader replica**) v√† nhi·ªÅu b·∫£n sao (**follower replica**) ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh s·∫µn s√†ng."
    },
    {
      "type": "text",
      "content": "**Ph√¢n b·ªï d·ªØ li·ªáu:**\nM·ªói partition s·∫Ω c√≥:\n- 1 **Leader Replica**: Producer s·∫Ω ghi v√† Consumer s·∫Ω ƒë·ªçc t·ª´ ƒë√¢y.\n- n **Follower Replicas**: B·∫£n sao n·∫±m tr√™n c√°c broker kh√°c, ch·ªâ d√πng ƒë·ªÉ d·ª± ph√≤ng.\n\n**V√≠ d·ª• ph√¢n chia d·ªØ li·ªáu theo partition:**\n- Partition 0 ‚Üí User 1 ƒë·∫øn 4\n- Partition 1 ‚Üí User 5 ƒë·∫øn 10\n\n**Note**: 1 job Spark c√≥ th·ªÉ **ƒë√≥ng vai tr√≤ consumer**, ƒë·ªçc d·ªØ li·ªáu t·ª´ Kafka v√† l∆∞u v√†o PostgreSQL ho·∫∑c Data Lake."
    },
    {
      "type": "image",
      "src": "/images/kafka/airchitechture.png",
      "alt": "Ki·∫øn tr√∫c t·ªïng th·ªÉ Kafka"
    }
  ]
},
{
  "id": "kafka-producer",
  "category": "kafka",
  "title": "Kafka Producer",
  "description": "Chi ti·∫øt c√°ch t·∫°o Kafka Producer, t·ªëi ∆∞u hi·ªáu su·∫•t v√† ƒë·∫£m b·∫£o ƒë·ªô tin c·∫≠y khi g·ª≠i d·ªØ li·ªáu.",
  "details": [
    {
      "type": "text",
      "content": "**Kafka Producer** l√† th√†nh ph·∫ßn ch·ªãu tr√°ch nhi·ªám g·ª≠i d·ªØ li·ªáu (message) v√†o m·ªôt Kafka Topic.\nProducer g·ª≠i d·ªØ li·ªáu tu·∫ßn t·ª± ho·∫∑c song song, c√≥ th·ªÉ t√πy ch·ªânh partition, batch, key ƒë·ªÉ ki·ªÉm so√°t hi·ªáu nƒÉng v√† t√≠nh ƒë√∫ng ƒë·∫Øn."
    },
    {
      "type": "code",
      "content": "from kafka import KafkaProducer\nproducer = KafkaProducer(bootstrap_servers='localhost:9092')\nproducer.send('my-topic', key=b'user-1', value=b'Hello Kafka!')\nproducer.flush()"
    },
    {
      "type": "text",
      "content": "###  **C√°c y·∫øu t·ªë quan tr·ªçng khi g·ª≠i d·ªØ li·ªáu**:\n\n- `key`: Gi√∫p **ph√¢n ph·ªëi message** v√†o c√πng partition n·∫øu c·∫ßn gi·ªØ th·ª© t·ª±.\n- `value`: D·ªØ li·ªáu th·ª±c t·∫ø (string, JSON, binary...).\n- `partition`: C√≥ th·ªÉ ch·ªâ ƒë·ªãnh th·ªß c√¥ng, m·∫∑c ƒë·ªãnh Kafka s·∫Ω ph√¢n ph·ªëi t·ª± ƒë·ªông."
    },
    {
      "type": "text",
      "content": "###  **flush()**\n- G·ª≠i to√†n b·ªô message ƒëang n·∫±m trong b·ªô nh·ªõ ƒë·ªám xu·ªëng Kafka broker.\n- N·∫øu kh√¥ng g·ªçi `flush()` ho·∫∑c `producer.close()`, ch∆∞∆°ng tr√¨nh c√≥ th·ªÉ k·∫øt th√∫c m√† ch∆∞a g·ª≠i h·∫øt d·ªØ li·ªáu."
    },
    {
      "type": "text",
      "content": "###  **Callback & Poll**\n- **Callback**: H√†m x·ª≠ l√Ω k·∫øt qu·∫£ g·ª≠i (th√†nh c√¥ng/th·∫•t b·∫°i).\n- **poll()**: B·∫Øt bu·ªôc ƒë·ªÉ Kafka x·ª≠ l√Ω callback (v·ªõi `confluent-kafka`).\n\n```python\nproducer.send('my-topic', b'data', callback=handle_callback)\nproducer.flush()\n```"
    },
    {
      "type": "text",
      "content": "###  **Partitioning theo Key**\n```python\nproducer.send('my-topic', key=b'account_001', value=b'transaction1')\n```\n‚û°Ô∏è T·∫•t c·∫£ message c√≥ c√πng key s·∫Ω v√†o **c√πng m·ªôt partition**, ƒë·∫£m b·∫£o th·ª© t·ª± (ordering)."
    }
  ]
}
,
{
  "id": "kafka-consumer",
  "category": "kafka",
  "title": "Kafka Consumer",
  "description": "C√°ch ho·∫°t ƒë·ªông c·ªßa consumer, group, offset v√† k·ªπ thu·∫≠t ƒë·∫£m b·∫£o x·ª≠ l√Ω an to√†n.",
  "details": [
    {
      "type": "text",
      "content": "**Kafka Consumer** l√† th√†nh ph·∫ßn ch·ªãu tr√°ch nhi·ªám **ƒë·ªçc d·ªØ li·ªáu t·ª´ Kafka topic**. N√≥ ho·∫°t ƒë·ªông d·ª±a tr√™n c∆° ch·∫ø group ƒë·ªÉ chia t·∫£i v√† qu·∫£n l√Ω ti·∫øn tr√¨nh x·ª≠ l√Ω."
    },
    {
      "type": "code",
      "content": "from kafka import KafkaConsumer\nconsumer = KafkaConsumer(\n    'my-topic',\n    bootstrap_servers='localhost:9092',\n    group_id='etl-group',\n    auto_offset_reset='earliest',\n    enable_auto_commit=True\n)\nfor msg in consumer:\n    print(msg.value.decode())"
    },
    {
      "type": "text",
      "content": "###  **Consumer Group**\n- C√°c consumer c√πng `group.id` s·∫Ω chia nhau c√°c partition ƒë·ªÉ x·ª≠ l√Ω song song.\n- M·ªói partition **ch·ªâ ƒë∆∞·ª£c ƒë·ªçc b·ªüi 1 consumer trong group t·∫°i m·ªôt th·ªùi ƒëi·ªÉm**.\n- Thay ƒë·ªïi s·ªë l∆∞·ª£ng consumer s·∫Ω g√¢y ra **rebalance**."
    },
    {
      "type": "text",
      "content": "###  **Offset ‚Äì Qu·∫£n l√Ω v·ªã tr√≠ ƒë·ªçc**\n- Kafka l∆∞u **offset** ƒë·ªÉ bi·∫øt consumer ƒë√£ ƒë·ªçc t·ªõi ƒë√¢u trong partition.\n- 2 c√°ch commit offset:\n  - `enable_auto_commit=True`: Kafka t·ª± commit sau m·ªói `poll()`.\n  - Manual commit: Khi b·∫°n x·ª≠ l√Ω xong m·ªõi `commit()` (an to√†n h∆°n).\n\n```python\nconsumer.commit()  # Khi x·ª≠ l√Ω th√†nh c√¥ng\n```"
    },
    {
      "type": "text",
      "content": "###  **Rebalance ‚Äì Ph√¢n ph·ªëi l·∫°i partition**\n- X·∫£y ra khi consumer join/leave group.\n- Kafka t·ª± ƒë·ªông ph√¢n ph·ªëi l·∫°i partition ‚Üí t·∫°m th·ªùi ng·∫Øt x·ª≠ l√Ω.\n- C√≥ th·ªÉ hook v√†o `on_partitions_revoked` v√† `on_partitions_assigned` ƒë·ªÉ x·ª≠ l√Ω khi rebalance x·∫£y ra."
    },
    {
      "type": "text",
      "content": "###  **Manual offset + Exactly-once**\n- K·∫øt h·ª£p `enable_auto_commit=False` v√† commit th·ªß c√¥ng **sau khi x·ª≠ l√Ω xong** gi√∫p tr√°nh m·∫•t m√°t/th·ª´a d·ªØ li·ªáu.\n- K·ªπ thu·∫≠t n√†y th∆∞·ªùng d√πng v·ªõi Spark, Flink, ho·∫∑c h·ªá th·ªëng l∆∞u tr·ªØ chu·∫©n nh∆∞ PostgreSQL, S3..."
    }
  ]
},
{
  "id": "kafka-consumer-advanced",
  "category": "kafka",
  "title": "Rebalance & Callback",
  "description": "X·ª≠ l√Ω rebalance b·∫±ng callback v√† ƒë·∫£m b·∫£o ƒë√∫ng logic ti√™u th·ª• d·ªØ li·ªáu Kafka.",
  "details": [
    {
      "type": "text",
      "content": "###  Rebalance l√† g√¨?\nRebalance x·∫£y ra khi c√≥ **consumer m·ªõi tham gia ho·∫∑c r·ªùi nh√≥m**, Kafka s·∫Ω ph√¢n ph·ªëi l·∫°i c√°c partition. \nTrong qu√° tr√¨nh n√†y, m·ªôt s·ªë partition t·∫°m th·ªùi kh√¥ng ƒë∆∞·ª£c x·ª≠ l√Ω ‚Üí n·∫øu kh√¥ng qu·∫£n l√Ω offset k·ªπ, c√≥ th·ªÉ g√¢y:\n- M·∫•t d·ªØ li·ªáu (ch∆∞a x·ª≠ l√Ω xong m√† b·ªã m·∫•t quy·ªÅn).\n- X·ª≠ l√Ω tr√πng (n·∫øu ch∆∞a commit offset)."
    },
    {
      "type": "text",
      "content": "###  C√°ch x·ª≠ l√Ω b·∫±ng callback: `on_partitions_revoked` v√† `on_partitions_assigned`\nD√πng callback ƒë·ªÉ ki·ªÉm so√°t lu·ªìng x·ª≠ l√Ω khi partition b·ªã l·∫•y ho·∫∑c ƒë∆∞·ª£c g√°n."
    },
    {
      "type": "code",
      "content": "from kafka import KafkaConsumer\nfrom kafka.consumer import TopicPartition\n\nclass RebalanceHandler:\n    def __init__(self, consumer):\n        self.consumer = consumer\n\n    def on_partitions_revoked(self, revoked):\n        print(f\"Partition b·ªã thu h·ªìi: {revoked}\")\n        # Commit offset th·ªß c√¥ng tr∆∞·ªõc khi m·∫•t quy·ªÅn x·ª≠ l√Ω\n        self.consumer.commit()\n\n    def on_partitions_assigned(self, assigned):\n        print(f\"Partition m·ªõi ƒë∆∞·ª£c g√°n: {assigned}\")"
    },
    {
      "type": "text",
      "content": "###  T√≠ch h·ª£p callback v√†o KafkaConsumer"
    },
    {
      "type": "code",
      "content": "consumer = KafkaConsumer(\n    'books',\n    bootstrap_servers='localhost:9092',\n    group_id='book-consumer-group',\n    enable_auto_commit=False,  # T·∫Øt t·ª± commit\n    auto_offset_reset='earliest'\n)\n\nhandler = RebalanceHandler(consumer)\n\nconsumer.subscribe(['books'],\n    on_assign=handler.on_partitions_assigned,\n    on_revoke=handler.on_partitions_revoked\n)"
    }

  ]
  }
,
{
  "id": "kafka-cli",
  "category": "kafka",
  "title": "Kafka CLI Commands",
  "description": "T·∫°o topic, g·ª≠i d·ªØ li·ªáu b·∫±ng producer v√† ƒë·ªçc d·ªØ li·ªáu b·∫±ng consumer trong Kafka CLI.",
  "details": [
    {
      "type": "text",
      "content": "###  T·∫°o Topic Kafka\nL·ªánh t·∫°o topic m·ªõi v·ªõi 1 partition v√† replication factor l√† 1:\n"
    },
    {
      "type": "code",
      "content": "/usr/bin/kafka-topics --create \\\n  --topic books \\\n  --bootstrap-server localhost:9092 \\\n  --partitions 1 \\\n  --replication-factor 1"
    },
    {
      "type": "text",
      "content": " **Xem danh s√°ch topic ƒë√£ t·∫°o:**"
    },
    {
      "type": "code",
      "content": "/usr/bin/kafka-topics --list --bootstrap-server localhost:9092"
    },
    {
      "type": "text",
      "content": "###  G·ª≠i Message v·ªõi Kafka Producer"
    },
    {
      "type": "text",
      "content": "####  G·ª≠i d·ªØ li·ªáu ƒë∆°n gi·∫£n:"
    },
    {
      "type": "code",
      "content": "/usr/bin/kafka-console-producer \\\n  --topic test_topic \\\n  --bootstrap-server localhost:9092"
    },
    {
      "type": "text",
      "content": " G√µ n·ªôi dung ‚Üí nh·∫•n Enter ƒë·ªÉ g·ª≠i, Ctrl+C ƒë·ªÉ d·ª´ng session."
    },
    {
      "type": "text",
      "content": "####  G·ª≠i message c√≥ key:"
    },
    {
      "type": "code",
      "content": "kafka-console-producer \\\n  --bootstrap-server localhost:9092 \\\n  --topic books \\\n  --property parse.key=true \\\n  --property key.separator=:"
    },
    {
      "type": "text",
      "content": " V√≠ d·ª• nh·∫≠p:\n```\nuser1:Harry Potter\nuser2:Sherlock Holmes\n```\nKafka s·∫Ω s·ª≠ d·ª•ng `user1`, `user2` l√†m key ‚Üí gi√∫p message c√πng key ƒëi v√†o c√πng m·ªôt partition."
    },
    {
      "type": "text",
      "content": "###  ƒê·ªçc d·ªØ li·ªáu v·ªõi Kafka Consumer"
    },
    {
      "type": "text",
      "content": "####  Hi·ªÉn th·ªã ƒë·∫ßy ƒë·ªß th√¥ng tin:"
    },
    {
      "type": "code",
      "content": "kafka-console-consumer \\\n  --bootstrap-server localhost:9092 \\\n  --topic bank_transactions \\\n  --from-beginning \\\n  --formatter kafka.tools.DefaultMessageFormatter \\\n  --property print.timestamp=true \\\n  --property print.key=true \\\n  --property print.value=true"
    },
    {
      "type": "text",
      "content": " Gi√∫p in ra: th·ªùi gian g·ª≠i, key, value c·ªßa t·ª´ng message."
    },
    {
      "type": "text",
      "content": "####  Ch·∫°y consumer trong container Docker:"
    },
    {
      "type": "code",
      "content": "docker exec -it kafka bash \\\n  /usr/bin/kafka-console-consumer \\\n  --topic bank_transactions \\\n  --from-beginning \\\n  --bootstrap-server localhost:9092"
    },
    {
      "type": "text",
      "content": "###  C√°c l·ªánh h·ªØu √≠ch kh√°c"
    },
    {
      "type": "text",
      "content": "####  Xo√° topic:"
    },
    {
      "type": "code",
      "content": "/usr/bin/kafka-topics --delete \\\n  --topic books \\\n  --bootstrap-server localhost:9092"
    },
    {
      "type": "text",
      "content": "#### M√¥ t·∫£ th√¥ng tin m·ªôt topic:"
    },
    {
      "type": "code",
      "content": "/usr/bin/kafka-topics --describe \\\n  --topic books \\\n  --bootstrap-server localhost:9092"
    }
  ]
}



]

