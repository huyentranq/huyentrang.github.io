[
{
  "id": "architecture",
  "category": "spark",
  "title": "Spark Architecture",
  "description": "Ki·∫øn tr√∫c t·ªïng quan c·ªßa Apache Spark, bao g·ªìm Driver, Executor, DAG Scheduler v√† Cluster Manager.",
  "details": [
        {
      "type": "image",
      "src": "/images/spark/architecture.png",
      "alt": "Ki·∫øn tr√∫c t·ªïng th·ªÉ Spark"
    },
    {
      "type": "text",
      "content": "Apache Spark l√† m·ªôt framework x·ª≠ l√Ω d·ªØ li·ªáu ph√¢n t√°n m·∫°nh m·∫Ω, ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ x·ª≠ l√Ω l∆∞·ª£ng l·ªõn d·ªØ li·ªáu m·ªôt c√°ch nhanh ch√≥ng v√† hi·ªáu qu·∫£. Ki·∫øn tr√∫c c·ªßa Spark bao g·ªìm c√°c th√†nh ph·∫ßn ch√≠nh sau:"
    },
    {
      "type": "text",
      "content": "Apache Spark s·ª≠ d·ª•ng ki·∫øn tr√∫c master-slave, trong ƒë√≥:\n- **Driver Program**: ƒëi·ªÅu khi·ªÉn to√†n b·ªô ti·∫øn tr√¨nh, t·∫°o `SparkContext`, g·ª≠i m√£ ch∆∞∆°ng tr√¨nh v√† gi√°m s√°t ti·∫øn ƒë·ªô.\n- **Cluster Manager**: qu·∫£n l√Ω t√†i nguy√™n (RAM, CPU) gi·ªØa c√°c node v√† ph√¢n b·ªï cho c√°c Executor.\n- **Executors**: ch·∫°y tr√™n Worker Node, th·ª±c thi c√°c task v√† l∆∞u tr·ªØ d·ªØ li·ªáu trung gian.\n- **DAG Scheduler & Task Scheduler**: DAG Scheduler chia nh·ªè job th√†nh c√°c stage, Task Scheduler ph√¢n chia th√†nh task v√† g·ª≠i t·ªõi c√°c Executor th·ª±c thi song song."
    },
    {
      "type": "text",
      "content": "### üîß Ki·∫øn tr√∫c th·ª±c thi n·ªôi b·ªô trong Apache Spark:\n1. **Driver Program**: ƒêi·ªÉm kh·ªüi ƒë·∫ßu c·ªßa ·ª©ng d·ª•ng, t·∫°o `SparkSession`, x√¢y d·ª±ng RDD, g·ª≠i Action (nh∆∞ `count()`) ƒë·ªÉ k√≠ch ho·∫°t job.\n2. **SparkContext**: Giao ti·∫øp gi·ªØa Driver v√† c√°c th√†nh ph·∫ßn b√™n d∆∞·ªõi, g·ª≠i DAG ƒë·∫øn DAG Scheduler.\n3. **DAG Scheduler**: Ph√¢n t√≠ch DAG, chia th√†nh stage, g·ª≠i ƒë·∫øn Task Scheduler.\n4. **Task Scheduler**: Chia nh·ªè stage th√†nh c√°c task theo partition v√† g·ª≠i ƒë·∫øn Cluster Manager.\n5. **Cluster Manager**: (YARN, Mesos, Kubernetes...) qu·∫£n l√Ω t√†i nguy√™n, ph√¢n b·ªï executor tr√™n c√°c node.\n6. **Executors**: Ch·∫°y task th·ª±c t·∫ø, l∆∞u d·ªØ li·ªáu trung gian v√† tr·∫£ k·∫øt qu·∫£ v·ªÅ cho Driver."
    },
    {
      "type": "text",
      "content": "**Lu·ªìng d·ªØ li·ªáu t·ªïng qu√°t:**\nDriver ‚Üí SparkContext ‚Üí DAG Scheduler ‚Üí Task Scheduler ‚Üí Cluster Manager ‚Üí Executor ‚Üí K·∫øt qu·∫£\n\n- Vi·ªác th·ª±c thi ƒë∆∞·ª£c b·∫Øt ƒë·∫ßu **ch·ªâ khi g·∫∑p Action**, nh·ªù v√†o c∆° ch·∫ø **Lazy Evaluation**.\n- DAG gi√∫p x√°c ƒë·ªãnh ch√≠nh x√°c c√°c ph√©p bi·∫øn ƒë·ªïi v√† t·ªëi ∆∞u h√≥a vi·ªác th·ª±c thi song song."
    },
    {
      "type": "code",
      "content": "SparkContext -> DAG Scheduler -> Task Scheduler -> Cluster Manager -> Executors"
    }

  ]
}
,
{
  "id": "concepts",
  "category": "spark",
  "title": "Spark Core Concepts",
  "description": "T·ªïng h·ª£p c√°c kh√°i ni·ªám c∆° b·∫£n v√† quan tr·ªçng trong Spark.",
  "details": [
    {
      "type": "text",
      "content": "- **RDD (Resilient Distributed Dataset)**: C·∫•u tr√∫c d·ªØ li·ªáu ph√¢n t√°n, b·∫•t bi·∫øn v√† h·ªó tr·ª£ thao t√°c song song. Cho ph√©p l·∫≠p tr√¨nh ph√¢n t√°n tr·ª±c ti·∫øp, nh∆∞ng kh√¥ng t·ªëi ∆∞u b·∫±ng DataFrame.\n\n- **DataFrame**: T·∫≠p d·ªØ li·ªáu d·∫°ng b·∫£ng c√≥ schema. D·ªÖ d√πng, h·ªó tr·ª£ API nh∆∞ SQL v√† ƒë∆∞·ª£c t·ªëi ∆∞u b·ªüi Catalyst optimizer. Th∆∞·ªùng d√πng thay RDD ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu hi·ªáu qu·∫£ h∆°n.\n\n- **Dataset** (Scala/Java): K·∫øt h·ª£p ∆∞u ƒëi·ªÉm c·ªßa RDD (type safety) v√† DataFrame (t·ªëi ∆∞u h√≥a). Kh√¥ng c√≥ trong PySpark.\n\n- **Transformation**: C√°c ph√©p bi·∫øn ƒë·ªïi lazy (kh√¥ng th·ª±c thi ngay), nh∆∞ `filter`, `map`, `select`, `groupBy`. Ch·ªâ t·∫°o DAG ƒë·ªÉ ch·ªù action.\n\n- **Action**: G·ªçi th·ª±c thi DAG, v√≠ d·ª• `show()`, `collect()`, `count()`, `take()`.\n\n- **Lazy Evaluation**: Spark tr√¨ ho√£n t√≠nh to√°n cho ƒë·∫øn khi c√≥ action, gi√∫p t·ªëi ∆∞u h√≥a pipeline x·ª≠ l√Ω.\n\n- **DAG (Directed Acyclic Graph)**: Spark t·∫°o DAG th·ªÉ hi·ªán lu·ªìng ph·ª• thu·ªôc gi·ªØa c√°c ph√©p bi·∫øn ƒë·ªïi. DAG Scheduler s·∫Ω chia DAG th√†nh c√°c stage.\n\n- **Stage & Task**: M·ªôt job ƒë∆∞·ª£c chia th√†nh nhi·ªÅu stage (t·∫≠p h·ª£p task). M·ªói task x·ª≠ l√Ω 1 partition d·ªØ li·ªáu. C√°c task ƒë∆∞·ª£c ph√¢n ph·ªëi song song ƒë·∫øn Executor.\n\n- **SparkSession & SparkContext**: `SparkSession` l√† entry point hi·ªán ƒë·∫°i cho m·ªçi thao t√°c Spark (DataFrame, SQL...). `SparkContext` l√† n·ªÅn t·∫£ng c≈© d√πng cho RDD.\n\n- **Cluster Manager**: Th√†nh ph·∫ßn qu·∫£n l√Ω t√†i nguy√™n c·ªßa cluster. Spark c√≥ th·ªÉ ch·∫°y tr√™n Standalone, YARN, Mesos ho·∫∑c Kubernetes."
    }
  ]
}
,
  {
    "id": "filter",
    "category": "pyspark",
    "title": "Filter DataFrame",
    "description": "L·ªçc d·ªØ li·ªáu theo ƒëi·ªÅu ki·ªán logic trong PySpark.",
    "details": [
      {
        "type": "code",
        "content": "df.filter(col('Item_Fat_Content') == 'Regular').show()"
      },
      {
        "type": "code",
        "content": "df.filter((col('Item_Type') == 'Soft Drinks') & (col('Item_Weight') < 10)).show()"
      },
      {
        "type": "code",
        "content": "df.filter((col('Outlet_Size').isNull()) & (col('Outlet_Location_Type').isin('Tier 1', 'Tier 2'))).show()"
      }
    ]
  },
  {
    "id": "udf",
    "category": "pyspark",
    "title": "User Defined Functions (UDF)",
    "description": "T·∫°o v√† s·ª≠ d·ª•ng h√†m t√πy ch·ªânh trong PySpark.",
    "details": [
      {
        "type": "text",
        "content": "Khi kh√¥ng th·ªÉ x·ª≠ l√Ω logic b·∫±ng h√†m c√≥ s·∫µn, b·∫°n c√≥ th·ªÉ ƒë·ªãnh nghƒ©a h√†m ri√™ng b·∫±ng UDF."
      },
      {
        "type": "code",
        "content": "from pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\nto_upper = udf(lambda x: x.upper(), StringType())\ndf.withColumn(\"upper_col\", to_upper(col(\"col_name\"))).show()"
      }
    ]
  },
  {
    "id": "select",
    "category": "pyspark",
    "title": "Select & Show",
    "description": "Ch·ªçn c·ªôt v√† hi·ªÉn th·ªã d·ªØ li·ªáu trong PySpark DataFrame.",
    "details": [
      {
        "type": "code",
        "content": "df.select('Item_Weight', 'Item_MRP').show()"
      },
      {
        "type": "code",
        "content": "df.select(col('Item_Weight'), col('Item_MRP')).show()"
      },
      {
        "type": "code",
        "content": "df.selectExpr('Item_Weight * Item_MRP as Total').show()"
      }
    ]
  },
{
  "id": "withcolumn",
  "category": "pyspark",
  "title": "WithColumn & Rename",
  "description": "Th√™m c·ªôt m·ªõi ho·∫∑c ƒë·ªïi t√™n c·ªôt trong PySpark DataFrame.",
  "details": [
    {
      "type": "code",
      "content": "// ƒê·ªïi t√™n c·ªôt 'Item_Weight' th√†nh 'Item_Wt'\ndf.withColumnRenamed('Item_Weight','Item_Wt').show()"
    },
    {
      "type": "code",
      "content": "// Th√™m c·ªôt m·ªõi 'multiply' l√† t√≠ch c·ªßa hai c·ªôt 'Item_Weight' v√† 'Item_MRP'\ndf.withColumn('multiply', col('Item_Weight') * col('Item_MRP')).show()"
    },
    {
      "type": "code",
      "content": "// Th√™m c·ªôt 'flag' c√≥ gi√° tr·ªã m·∫∑c ƒë·ªãnh l√† chu·ªói 'new'\ndf.withColumn('flag', lit('new')).show()"
    },
    {
      "type": "code",
      "content": "// Thay th·∫ø gi√° tr·ªã trong c·ªôt 'Item_Fat_Content':\n// 'Regular' -> 'Reg', 'Low Fat' -> 'Lf'\ndf = df.withColumn('Item_Fat_Content', regexp_replace(col('Item_Fat_Content'), 'Regular', 'Reg'))\n         .withColumn('Item_Fat_Content', regexp_replace(col('Item_Fat_Content'), 'Low Fat', 'Lf'))"
    }
  ]
}
,
  {
    "id": "groupby",
    "category": "pyspark",
    "title": "GroupBy & Aggregation",
    "description": "S·ª≠ d·ª•ng groupBy v√† c√°c h√†m t·ªïng h·ª£p trong PySpark.",
    "details": [
      {
        "type": "code",
        "content": "df.groupBy(\"column\").agg(count(\"*\"), sum(\"value\")).show()"
      },
      {
        "type": "code",
        "content": "df.groupBy(\"column\").agg(avg(\"value\"), min(\"value\"), max(\"value\")).show()"
      },
      {
        "type": "code",
        "content": "df.groupBy(\"column\").agg(collect_list(\"value\"), collect_set(\"value\")).show()"
      },
      {
        "type": "image",
        "src": "/images/spark-groupby.png",
        "alt": "GroupBy trong PySpark"
      }
    ]
  },
 {
  "id": "join",
  "category": "pyspark",
  "title": "Join DataFrames",
  "description": "K·∫øt h·ª£p hai DataFrame b·∫±ng c√°c lo·∫°i join kh√°c nhau trong PySpark.",
  "details": [
    {
      "type": "text",
      "content": "PySpark h·ªó tr·ª£ nhi·ªÅu lo·∫°i `join` nh∆∞:\n\n- **inner**: Ch·ªâ gi·ªØ l·∫°i c√°c d√≤ng c√≥ kh√≥a tr√πng kh·ªõp ·ªü c·∫£ hai DataFrame.\n- **left** (left outer): Gi·ªØ l·∫°i t·∫•t c·∫£ d√≤ng c·ªßa DataFrame b√™n tr√°i, k·∫øt h·ª£p d·ªØ li·ªáu t·ª´ b√™n ph·∫£i n·∫øu c√≥.\n- **right** (right outer): T∆∞∆°ng t·ª± nh∆∞ left, nh∆∞ng gi·ªØ l·∫°i to√†n b·ªô d√≤ng t·ª´ DataFrame b√™n ph·∫£i.\n- **outer** (full outer): Gi·ªØ l·∫°i t·∫•t c·∫£ d√≤ng t·ª´ c·∫£ hai b·∫£ng, ph·∫ßn kh√¥ng kh·ªõp s·∫Ω c√≥ gi√° tr·ªã `null`.\n- **left_semi**: Gi·ªØ l·∫°i c√°c d√≤ng t·ª´ DataFrame b√™n tr√°i m√† c√≥ kh√≥a kh·ªõp v·ªõi b√™n ph·∫£i, nh∆∞ng **kh√¥ng** l·∫•y c·ªôt t·ª´ b√™n ph·∫£i.\n- **left_anti**: Gi·ªØ l·∫°i c√°c d√≤ng t·ª´ b√™n tr√°i m√† **kh√¥ng** c√≥ kh√≥a kh·ªõp v·ªõi b√™n ph·∫£i."
    },
    {
      "type": "code",
      "content": "df1.join(df2, df1.id == df2.id, 'inner').show()  # inner join"
    },
    {
      "type": "code",
      "content": "df1.join(df2, df1.id == df2.id, 'left').show()  # left outer join"
    },
    {
      "type": "code",
      "content": "df1.join(df2, df1.id == df2.id, 'right').show()  # right outer join"
    },
    {
      "type": "code",
      "content": "df1.join(df2, df1.id == df2.id, 'outer').show()  # full outer join"
    },
    {
      "type": "code",
      "content": "df1.join(df2, df1.id == df2.id, 'left_semi').show()  # ch·ªâ l·∫•y c·ªôt t·ª´ df1"
    },
    {
      "type": "code",
      "content": "df1.join(df2, df1.id == df2.id, 'left_anti').show()  # d√≤ng kh√¥ng kh·ªõp t·ª´ df1"
    },
    {
      "type": "text",
      "content": "**L∆∞u √Ω:** C·ªôt `id` ph·∫£i c√≥ c√πng t√™n ho·∫∑c c·∫ßn ƒë∆∞·ª£c tham chi·∫øu r√µ r√†ng (`df1.id == df2.id`). Trong tr∆∞·ªùng h·ª£p c·ªôt b·ªã tr√πng t√™n, n√™n d√πng `.alias()` ƒë·ªÉ tr√°nh xung ƒë·ªôt c·ªôt sau khi join."
    }
  ]
}
,
  {
    "id": "drop",
    "category": "pyspark",
    "title": "Drop & Remove Duplicates",
    "description": "X√≥a c·ªôt v√† lo·∫°i b·ªè b·∫£n ghi tr√πng l·∫∑p trong DataFrame.",
    "details": [
      {
        "type": "code",
        "content": "df.drop('Item_Visibility').show()"
      },
      {
        "type": "code",
        "content": "df.drop('Item_Visibility', 'Item_Type').show()"
      },
      {
        "type": "code",
        "content": "df.dropDuplicates().show()"
      },
      {
        "type": "code",
        "content": "df.drop_duplicates(subset=['Item_Type']).show()"
      },
      {
        "type": "code",
        "content": "df.distinct().show()"
      }
    ]
  },
  {
    "id": "spark-sql",
    "category": "pyspark",
    "title": "Spark SQL",
    "description": "S·ª≠ d·ª•ng SQL ƒë·ªÉ truy v·∫•n d·ªØ li·ªáu trong DataFrame.",
    "details": [
      {
        "type": "text",
        "content": "Spark SQL cho ph√©p b·∫°n ƒëƒÉng k√Ω DataFrame nh∆∞ m·ªôt b·∫£ng t·∫°m th·ªùi v√† s·ª≠ d·ª•ng SQL ƒë·ªÉ truy v·∫•n."
      },
      {
        "type": "code",
        "content": "df.createOrReplaceTempView(\"my_table\")\nspark.sql(\"SELECT * FROM my_table WHERE column = 'value'\").show()"
      }
    ]
  }
]
